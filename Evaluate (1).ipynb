{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Evaluate.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sgoGpyjphyf2",
        "fQYfdcUKXx1C",
        "QyCh_mhIt3Rc",
        "GY2DPOnEy3LV",
        "HHDwGUMyzEY2",
        "ABfa4b44784Q",
        "yJN_m6WemzJ5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk9-yqi1xZWJ"
      },
      "source": [
        "#A notebook implementation of Keras-Retinanet Evalute.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgoGpyjphyf2"
      },
      "source": [
        "#Downgrade Tensorflow and Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0t-M3QAT2rS",
        "outputId": "1d827aa7-969a-46c9-82cc-6e9c8b50077c"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FWx8egUZptM"
      },
      "source": [
        "!pip uninstall keras-nightly\n",
        "\n",
        "!pip uninstall -y tensorflow\n",
        "\n",
        "!pip install h5py==2.10.0  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hd_PvMnUXk3"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip uninstall keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWZunrewUtm_"
      },
      "source": [
        "!pip install tensorflow==2.3.0\n",
        "!pip install keras==2.4.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQYfdcUKXx1C"
      },
      "source": [
        "#Download keras-retinannet and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4dOT5rAuJnu"
      },
      "source": [
        "!git clone https://github.com/SherilK/keras-retinanet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY8Deq3muMLt"
      },
      "source": [
        "%cd keras-retinanet/\n",
        "\n",
        "!pip install keras_retinanet\n",
        "!pip install keras-resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxcI_VbKuPnI"
      },
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyCh_mhIt3Rc"
      },
      "source": [
        "#EVALUATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPEkNHN2s_dw"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Allow relative imports when being executed as script.\n",
        "'''\n",
        "if __name__ == \"__main__\" and __package__ is None:\n",
        "    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))\n",
        "    import keras_retinanet.bin  # noqa: F401\n",
        "    __package__ = \"keras_retinanet.bin\"\n",
        "'''\n",
        "# Change these to absolute imports if you copy this script outside the keras_retinanet package.\n",
        "from keras_retinanet import models\n",
        "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
        "from keras_retinanet.preprocessing.pascal_voc import PascalVocGenerator\n",
        "from keras_retinanet.utils.anchors import make_shapes_callback\n",
        "from keras_retinanet.utils.config import read_config_file, parse_anchor_parameters, parse_pyramid_levels\n",
        "#from keras_retinanet.utils.eval import evaluate\n",
        "from keras_retinanet.utils.gpu import setup_gpu\n",
        "from keras_retinanet.utils.tf_version import check_tf_version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ZXST7AiKJz"
      },
      "source": [
        "from keras_retinanet.utils.anchors import compute_overlap\n",
        "from keras_retinanet.utils.visualization import draw_detections, draw_annotations\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import progressbar\n",
        "assert(callable(progressbar.progressbar)), \"Using wrong progressbar module, install 'progressbar2' instead.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfnQlwnZ1ZYL"
      },
      "source": [
        "\n",
        "from  keras_retinanet.utils.colors import label_color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY2DPOnEy3LV"
      },
      "source": [
        "#Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omEU3x37y11x"
      },
      "source": [
        "\n",
        "def draw_caption(image, box, caption):\n",
        "    \"\"\" Draws a caption above the box in an image.\n",
        "\n",
        "    # Arguments\n",
        "        image   : The image to draw on.\n",
        "        box     : A list of 4 elements (x1, y1, x2, y2).\n",
        "        caption : String containing the text to draw.\n",
        "    \"\"\"\n",
        "    b = np.array(box).astype(int)\n",
        "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)\n",
        "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2qAtxdPyxSG"
      },
      "source": [
        "\n",
        "def draw_box(image, box, color, thickness=2):\n",
        "    \"\"\" Draws a box on an image with a given color.\n",
        "\n",
        "    # Arguments\n",
        "        image     : The image to draw on.\n",
        "        box       : A list of 4 elements (x1, y1, x2, y2).\n",
        "        color     : The color of the box.\n",
        "        thickness : The thickness of the lines to draw a box with.\n",
        "    \"\"\"\n",
        "    b = np.array(box).astype(int)\n",
        "    cv2.rectangle(image, (b[0], b[1]), (b[2], b[3]), color, thickness, cv2.LINE_AA)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzbuPFVAyrI9"
      },
      "source": [
        "\n",
        "def draw_detections(image, boxes, scores, labels, color=None, label_to_name=None, score_threshold=0.5):\n",
        "    \"\"\" Draws detections in an image.\n",
        "\n",
        "    # Arguments\n",
        "        image           : The image to draw on.\n",
        "        boxes           : A [N, 4] matrix (x1, y1, x2, y2).\n",
        "        scores          : A list of N classification scores.\n",
        "        labels          : A list of N labels.\n",
        "        color           : The color of the boxes. By default the color from keras_retinanet.utils.colors.label_color will be used.\n",
        "        label_to_name   : (optional) Functor for mapping a label to a name.\n",
        "        score_threshold : Threshold used for determining what detections to draw.\n",
        "    \"\"\"\n",
        "    selection = np.where(scores > score_threshold)[0]\n",
        "\n",
        "    for i in selection:\n",
        "        c = color if color is not None else label_color(labels[i])\n",
        "        draw_box(image, boxes[i, :], color=c)\n",
        "\n",
        "        # draw labels\n",
        "        caption = (label_to_name(labels[i]) if label_to_name else labels[i]) + ': {0:.2f}'.format(scores[i])\n",
        "        draw_caption(image, boxes[i, :], caption)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedggRwZynUl",
        "outputId": "35f30073-16b9-4788-f7d4-7970cc194cd8"
      },
      "source": [
        "'''\n",
        "def draw_annotations(image, annotations, color=(0, 255, 0), label_to_name=None):\n",
        "    \"\"\" Draws annotations in an image.\n",
        "\n",
        "    # Arguments\n",
        "        image         : The image to draw on.\n",
        "        annotations   : A [N, 5] matrix (x1, y1, x2, y2, label) or dictionary containing bboxes (shaped [N, 4]) and labels (shaped [N]).\n",
        "        color         : The color of the boxes. By default the color from keras_retinanet.utils.colors.label_color will be used.\n",
        "        label_to_name : (optional) Functor for mapping a label to a name.\n",
        "    \"\"\"\n",
        "    if isinstance(annotations, np.ndarray):\n",
        "        annotations = {'bboxes': annotations[:, :4], 'labels': annotations[:, 4]}\n",
        "\n",
        "    assert('bboxes' in annotations)\n",
        "    assert('labels' in annotations)\n",
        "    assert(annotations['bboxes'].shape[0] == annotations['labels'].shape[0])\n",
        "\n",
        "    for i in range(annotations['bboxes'].shape[0]):\n",
        "        label   = annotations['labels'][i]\n",
        "        c       = color if color is not None else label_color(label)\n",
        "        caption = '{}'.format(label_to_name(label) if label_to_name else label)\n",
        "        draw_caption(image, annotations['bboxes'][i], caption)\n",
        "        draw_box(image, annotations['bboxes'][i], color=c)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef draw_annotations(image, annotations, color=(0, 255, 0), label_to_name=None):\\n    \"\"\" Draws annotations in an image.\\n\\n    # Arguments\\n        image         : The image to draw on.\\n        annotations   : A [N, 5] matrix (x1, y1, x2, y2, label) or dictionary containing bboxes (shaped [N, 4]) and labels (shaped [N]).\\n        color         : The color of the boxes. By default the color from keras_retinanet.utils.colors.label_color will be used.\\n        label_to_name : (optional) Functor for mapping a label to a name.\\n    \"\"\"\\n    if isinstance(annotations, np.ndarray):\\n        annotations = {\\'bboxes\\': annotations[:, :4], \\'labels\\': annotations[:, 4]}\\n\\n    assert(\\'bboxes\\' in annotations)\\n    assert(\\'labels\\' in annotations)\\n    assert(annotations[\\'bboxes\\'].shape[0] == annotations[\\'labels\\'].shape[0])\\n\\n    for i in range(annotations[\\'bboxes\\'].shape[0]):\\n        label   = annotations[\\'labels\\'][i]\\n        c       = color if color is not None else label_color(label)\\n        caption = \\'{}\\'.format(label_to_name(label) if label_to_name else label)\\n        draw_caption(image, annotations[\\'bboxes\\'][i], caption)\\n        draw_box(image, annotations[\\'bboxes\\'][i], color=c)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4mFr2FVClyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea166258-2c0d-4544-a24c-da6eb1ee5a70"
      },
      "source": [
        "'''\n",
        "def _get_annotations(generator):\n",
        "    \"\"\" Get the ground truth annotations from the generator.\n",
        "\n",
        "    The result is a list of lists such that the size is:\n",
        "        all_detections[num_images][num_classes] = annotations[num_detections, 5]\n",
        "\n",
        "    # Arguments\n",
        "        generator : The generator used to retrieve ground truth annotations.\n",
        "    # Returns\n",
        "        A list of lists containing the annotations for each image in the generator.\n",
        "    \"\"\"\n",
        "    all_annotations = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
        "\n",
        "    for i in progressbar.progressbar(range(generator.size()), prefix='Parsing annotations: '):\n",
        "        # load the annotations\n",
        "        annotations = generator.load_annotations(i)\n",
        "\n",
        "        # copy detections to all_annotations\n",
        "        for label in range(generator.num_classes()):\n",
        "            if not generator.has_label(label):\n",
        "                continue\n",
        "\n",
        "            all_annotations[i][label] = annotations['bboxes'][annotations['labels'] == label, :].copy()\n",
        "\n",
        "    return all_annotations\n",
        "'''    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef _get_annotations(generator):\\n    \"\"\" Get the ground truth annotations from the generator.\\n\\n    The result is a list of lists such that the size is:\\n        all_detections[num_images][num_classes] = annotations[num_detections, 5]\\n\\n    # Arguments\\n        generator : The generator used to retrieve ground truth annotations.\\n    # Returns\\n        A list of lists containing the annotations for each image in the generator.\\n    \"\"\"\\n    all_annotations = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\\n\\n    for i in progressbar.progressbar(range(generator.size()), prefix=\\'Parsing annotations: \\'):\\n        # load the annotations\\n        annotations = generator.load_annotations(i)\\n\\n        # copy detections to all_annotations\\n        for label in range(generator.num_classes()):\\n            if not generator.has_label(label):\\n                continue\\n\\n            all_annotations[i][label] = annotations[\\'bboxes\\'][annotations[\\'labels\\'] == label, :].copy()\\n\\n    return all_annotations\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inzhGH9EzAZO"
      },
      "source": [
        "#Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV-a_ER1heNx"
      },
      "source": [
        "def evaluate(\n",
        "    generator,\n",
        "    model,\n",
        "    iou_threshold=0.5,\n",
        "    score_threshold=0.05,\n",
        "    max_detections=100,\n",
        "    save_path=None\n",
        "):\n",
        "    \"\"\" Evaluate a given dataset using a given model.\n",
        "\n",
        "    # Arguments\n",
        "        generator       : The generator that represents the dataset to evaluate.\n",
        "        model           : The model to evaluate.\n",
        "        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
        "        score_threshold : The score confidence threshold to use for detections.\n",
        "        max_detections  : The maximum number of detections to use per image.\n",
        "        save_path       : The path to save images with visualized detections to.\n",
        "    # Returns\n",
        "        A dict mapping class names to mAP scores.\n",
        "    \"\"\"\n",
        "    # gather all detections and annotations\n",
        "    all_detections, all_inferences = _get_detections(generator, model, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)\n",
        "    all_annotations    = _get_annotations(generator)\n",
        "    average_precisions = {}\n",
        "\n",
        "    # all_detections = pickle.load(open('all_detections.pkl', 'rb'))\n",
        "    # all_annotations = pickle.load(open('all_annotations.pkl', 'rb'))\n",
        "    # pickle.dump(all_detections, open('all_detections.pkl', 'wb'))\n",
        "    # pickle.dump(all_annotations, open('all_annotations.pkl', 'wb'))\n",
        "\n",
        "    # process detections and annotations\n",
        "    for label in range(generator.num_classes()-1):\n",
        "        if not generator.has_label(label):\n",
        "            continue\n",
        "\n",
        "        false_positives = np.zeros((0,))\n",
        "        true_positives  = np.zeros((0,))\n",
        "        scores          = np.zeros((0,))\n",
        "        num_annotations = 0.0\n",
        "\n",
        "        for i in range(generator.size()):\n",
        "            detections           = all_detections[i][label]\n",
        "            annotations          = all_annotations[i][label]\n",
        "            num_annotations     += annotations.shape[0]\n",
        "            detected_annotations = []\n",
        "\n",
        "            for d in detections:\n",
        "                scores = np.append(scores, d[4])\n",
        "\n",
        "                if annotations.shape[0] == 0:\n",
        "                    false_positives = np.append(false_positives, 1)\n",
        "                    true_positives  = np.append(true_positives, 0)\n",
        "                    continue\n",
        "\n",
        "                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
        "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
        "                max_overlap         = overlaps[0, assigned_annotation]\n",
        "\n",
        "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
        "                    false_positives = np.append(false_positives, 0)\n",
        "                    true_positives  = np.append(true_positives, 1)\n",
        "                    detected_annotations.append(assigned_annotation)\n",
        "                else:\n",
        "                    false_positives = np.append(false_positives, 1)\n",
        "                    true_positives  = np.append(true_positives, 0)\n",
        "\n",
        "        # no annotations -> AP for this class is 0 (is this correct?)\n",
        "        if num_annotations == 0:\n",
        "            average_precisions[label] = 0, 0\n",
        "            continue\n",
        "\n",
        "        # sort by score\n",
        "        indices         = np.argsort(-scores)\n",
        "        false_positives = false_positives[indices]\n",
        "        true_positives  = true_positives[indices]\n",
        "\n",
        "        # compute false positives and true positives\n",
        "        false_positives = np.cumsum(false_positives)\n",
        "        true_positives  = np.cumsum(true_positives)\n",
        "\n",
        "        # compute recall and precision\n",
        "        recall    = true_positives / num_annotations\n",
        "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
        "\n",
        "        # compute average precision\n",
        "        average_precision  = _compute_ap(recall, precision)\n",
        "        average_precisions[label] = average_precision, num_annotations\n",
        "\n",
        "    # inference time\n",
        "    inference_time = np.sum(all_inferences) / generator.size()\n",
        "\n",
        "    return average_precisions, inference_time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG3SZS8uhF7h"
      },
      "source": [
        "def _get_annotations(generator):\n",
        "    \"\"\" Get the ground truth annotations from the generator.\n",
        "\n",
        "    The result is a list of lists such that the size is:\n",
        "        all_detections[num_images][num_classes] = annotations[num_detections, 5]\n",
        "\n",
        "    # Arguments\n",
        "        generator : The generator used to retrieve ground truth annotations.\n",
        "    # Returns\n",
        "        A list of lists containing the annotations for each image in the generator.\n",
        "    \"\"\"\n",
        "    all_annotations = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
        "\n",
        "    for i in progressbar.progressbar(range(generator.size()), prefix='Parsing annotations: '):\n",
        "        # load the annotations\n",
        "        annotations = generator.load_annotations(i)\n",
        "\n",
        "        # copy detections to all_annotations\n",
        "        for label in range(generator.num_classes()-1):\n",
        "            if not generator.has_label(label):\n",
        "                continue\n",
        "\n",
        "            all_annotations[i][label] = annotations['bboxes'][annotations['labels'] == label, :].copy()\n",
        "\n",
        "    return all_annotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVsB-WIGhyKr"
      },
      "source": [
        "def _get_detections(generator, model, score_threshold=0.05, max_detections=100, save_path=None):\n",
        "    \"\"\" Get the detections from the model using the generator.\n",
        "\n",
        "    The result is a list of lists such that the size is:\n",
        "        all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes]\n",
        "\n",
        "    # Arguments\n",
        "        generator       : The generator used to run images through the model.\n",
        "        model           : The model to run on the images.\n",
        "        score_threshold : The score confidence threshold to use.\n",
        "        max_detections  : The maximum number of detections to use per image.\n",
        "        save_path       : The path to save the images with visualized detections to.\n",
        "    # Returns\n",
        "        A list of lists containing the detections for each image in the generator.\n",
        "    \"\"\"\n",
        "    #save_path = None\n",
        "    all_detections = [[None for i in range(generator.num_classes()) if generator.has_label(i)] for j in range(generator.size())]\n",
        "    all_inferences = [None for i in range(generator.size())]\n",
        "\n",
        "    for i in progressbar.progressbar(range(generator.size()), prefix='Running network: '):\n",
        "        print(\"i\",i)\n",
        "        raw_image    = generator.load_image(i)\n",
        "        image, scale = generator.resize_image(raw_image.copy())\n",
        "        image = generator.preprocess_image(image)\n",
        "\n",
        "        if keras.backend.image_data_format() == 'channels_first':\n",
        "            image = image.transpose((2, 0, 1))\n",
        "\n",
        "        # run network\n",
        "        start = time.time()\n",
        "        boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))[:3]\n",
        "        inference_time = time.time() - start\n",
        "\n",
        "        # correct boxes for image scale\n",
        "        boxes /= scale\n",
        "\n",
        "        # select indices which have a score above the threshold\n",
        "        indices = np.where(scores[0, :] > score_threshold)[0]\n",
        "\n",
        "        # select those scores\n",
        "        scores = scores[0][indices]\n",
        "\n",
        "        # find the order with which to sort the scores\n",
        "        scores_sort = np.argsort(-scores)[:max_detections]\n",
        "\n",
        "        # select detections\n",
        "        image_boxes      = boxes[0, indices[scores_sort], :]\n",
        "        image_scores     = scores[scores_sort]\n",
        "        image_labels     = labels[0, indices[scores_sort]]\n",
        "        image_detections = np.concatenate([image_boxes, np.expand_dims(image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)\n",
        "\n",
        "        if save_path is not None:\n",
        "            draw_annotations(raw_image, generator.load_annotations(i), label_to_name=generator.label_to_name)\n",
        "            draw_detections(raw_image, image_boxes, image_scores, image_labels, label_to_name=generator.label_to_name, score_threshold=score_threshold)\n",
        "\n",
        "            #cv2.imwrite(os.path.join(save_path, '{}.png'.format(i)), raw_image)\n",
        "\n",
        "            #CHANGE HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "            \n",
        "            draw = image.copy()\n",
        "            draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
        "            #loop\n",
        "            _annotations = generator.load_annotations(i)\n",
        "            bounding_boxes = _annotations.get('bboxes')\n",
        "\n",
        "            for b in bounding_boxes:\n",
        "              true_box = b\n",
        "              draw_box(draw, true_box, color=(255, 255, 0))\n",
        "\n",
        "            THRES_SCORE = 0.5  \n",
        "            draw__detections(draw, boxes, scores, labels, generator)\n",
        "            cv2.imwrite(os.path.join(save_path, '{}.png'.format(i)), raw_image)\n",
        "            \n",
        "            ###################################################################\n",
        "\n",
        "\n",
        "        # copy detections to all_detections\n",
        "        for label in range(generator.num_classes()-1):\n",
        "          if not generator.has_label(label):\n",
        "            continue\n",
        "          all_detections[i][label] = image_detections[image_detections[:, -1] == label, :-1]\n",
        "          print(\"label\",label)\n",
        "        all_inferences[i] = inference_time\n",
        "\n",
        "    return all_detections, all_inferences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWrNGvMWIrDY"
      },
      "source": [
        "def _compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list).\n",
        "        precision: The precision curve (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.], recall, [1.]))\n",
        "    mpre = np.concatenate(([0.], precision, [0.]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHDwGUMyzEY2"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fhfdreMt_UO"
      },
      "source": [
        "def create_generator(image_min_side, image_max_side, dataset_type, annotations , classes, config, preprocess_image):\n",
        "    \"\"\" Create generators for evaluation.\n",
        "    \"\"\"\n",
        "    no_resize = False \n",
        "    group_method = 'ratio'\n",
        "    common_args = {\n",
        "        'config'           : config,\n",
        "        'image_min_side'   : image_min_side,\n",
        "        'image_max_side'   : image_max_side,\n",
        "        'no_resize'        : no_resize,\n",
        "        'preprocess_image' : preprocess_image,\n",
        "        'group_method'     : group_method\n",
        "    }\n",
        "  \n",
        "\n",
        "    if dataset_type == 'coco':\n",
        "        # import here to prevent unnecessary dependency on cocoapi\n",
        "        from ..preprocessing.coco import CocoGenerator\n",
        "\n",
        "        validation_generator = CocoGenerator(\n",
        "            args.coco_path,\n",
        "            'val2017',\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    elif dataset_type == 'pascal':\n",
        "        validation_generator = PascalVocGenerator(\n",
        "            args.pascal_path,\n",
        "            'test',\n",
        "            image_extension=args.image_extension,\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    elif dataset_type == 'csv':\n",
        "        validation_generator = CSVGenerator(\n",
        "            annotations,\n",
        "            classes,\n",
        "            shuffle_groups=False,\n",
        "            **common_args\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))\n",
        "\n",
        "    return validation_generator\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF04PEIGwbTx"
      },
      "source": [
        "def parse_args(args):\n",
        "    \"\"\" Parse the arguments.\n",
        "    \"\"\"\n",
        "    parser     = argparse.ArgumentParser(description='Evaluation script for a RetinaNet network.')\n",
        "    subparsers = parser.add_subparsers(help='Arguments for specific dataset types.', dest='dataset_type')\n",
        "    subparsers.required = True\n",
        "\n",
        "    coco_parser = subparsers.add_parser('coco')\n",
        "    coco_parser.add_argument('coco_path', help='Path to dataset directory (ie. /tmp/COCO).')\n",
        "\n",
        "    pascal_parser = subparsers.add_parser('pascal')\n",
        "    pascal_parser.add_argument('pascal_path', help='Path to dataset directory (ie. /tmp/VOCdevkit).')\n",
        "    pascal_parser.add_argument('--image-extension',   help='Declares the dataset images\\' extension.', default='.jpg')\n",
        "\n",
        "    csv_parser = subparsers.add_parser('csv')\n",
        "    csv_parser.add_argument('annotations', help='Path to CSV file containing annotations for evaluation.')\n",
        "    csv_parser.add_argument('classes', help='Path to a CSV file containing class label mapping.')\n",
        "\n",
        "    parser.add_argument('model',              help='Path to RetinaNet model.')\n",
        "    parser.add_argument('--convert-model',    help='Convert the model to an inference model (ie. the input is a training model).', action='store_true')\n",
        "    parser.add_argument('--backbone',         help='The backbone of the model.', default='resnet50')\n",
        "    parser.add_argument('--gpu',              help='Id of the GPU to use (as reported by nvidia-smi).')\n",
        "    parser.add_argument('--score-threshold',  help='Threshold on score to filter detections with (defaults to 0.05).', default=0.05, type=float)\n",
        "    parser.add_argument('--iou-threshold',    help='IoU Threshold to count for a positive detection (defaults to 0.5).', default=0.5, type=float)\n",
        "    parser.add_argument('--max-detections',   help='Max Detections per image (defaults to 100).', default=100, type=int)\n",
        "    parser.add_argument('--save-path',        help='Path for saving images with detections (doesn\\'t work for COCO).')\n",
        "    parser.add_argument('--image-min-side',   help='Rescale the image so the smallest side is min_side.', type=int, default=800)\n",
        "    parser.add_argument('--image-max-side',   help='Rescale the image if the largest side is larger than max_side.', type=int, default=1333)\n",
        "    parser.add_argument('--no-resize',        help='Don''t rescale the image.', action='store_true')\n",
        "    parser.add_argument('--config',           help='Path to a configuration parameters .ini file (only used with --convert-model).')\n",
        "    parser.add_argument('--group-method',     help='Determines how images are grouped together', type=str, default='ratio', choices=['none', 'random', 'ratio'])\n",
        "\n",
        "    return parser.parse_args(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8XekGa3ugWF"
      },
      "source": [
        "def main(dataset_type, annotations, classes, model_path, save_path):\n",
        "  #backbone_name = 'resnet50'\n",
        "  image_min_side = 800\n",
        "  image_max_side = 1333\n",
        "  convert_model = True\n",
        "  # parse arguments\n",
        "  '''\n",
        "  if args is None:\n",
        "    args = sys.argv[1:]\n",
        "    args = parse_args(args)\n",
        "  '''\n",
        "\n",
        "  # make sure tensorflow is the minimum required version\n",
        "  check_tf_version()\n",
        "\n",
        "  # optionally choose specific GPU\n",
        "  '''\n",
        "  if args.gpu:\n",
        "    setup_gpu(args.gpu)\n",
        "  '''\n",
        "  # make save path if it doesn't exist\n",
        "\n",
        "  if save_path is not None and not os.path.exists(save_path):\n",
        "      os.makedirs(args.save_path)\n",
        "\n",
        "  # optionally load config parameters\n",
        "  config = '/content/Config/config.ini'\n",
        "  \n",
        "  if config:\n",
        "    config = read_config_file(config)\n",
        "  \n",
        "  # create the generator\n",
        "  backbone = models.backbone('resnet50')\n",
        "  generator = create_generator(image_min_side, image_max_side, dataset_type, annotations , classes, config,backbone.preprocess_image)\n",
        "\n",
        "  # optionally load anchor parameters\n",
        "  anchor_params = None\n",
        "  pyramid_levels = None\n",
        "\n",
        "  if config and 'anchor_parameters' in config:\n",
        "      anchor_params = parse_anchor_parameters(config)\n",
        "  '''\n",
        "  if args.config and 'pyramid_levels' in args.config:\n",
        "      pyramid_levels = parse_pyramid_levels(args.config)\n",
        "  '''\n",
        "  # load the model\n",
        "  print('Loading model, this may take a second...')\n",
        "  model = models.load_model(model_path, backbone_name='resnet50')\n",
        "  generator.compute_shapes = make_shapes_callback(model)\n",
        "\n",
        "  # optionally convert the model\n",
        "  if convert_model:\n",
        "    model = models.convert_model(model, anchor_params=anchor_params, pyramid_levels=pyramid_levels)\n",
        "    #model = models.convert_model(model)\n",
        "\n",
        "  # print model summary\n",
        "  #print(model.summary())\n",
        "  print(generator.num_classes())\n",
        "  # start evaluation\n",
        "  \n",
        "  #print(generator.load_annotations(19));\n",
        "  \n",
        "\n",
        "  if dataset_type == 'coco':\n",
        "    from keras_retinanet.utils.coco_eval import evaluate_coco\n",
        "    evaluate_coco(generator, model, args.score_threshold)\n",
        "  else:\n",
        "    average_precisions, inference_time = evaluate(\n",
        "        generator,\n",
        "        model,\n",
        "        iou_threshold=0.5,\n",
        "        score_threshold= 0.05,\n",
        "        max_detections=100,\n",
        "        save_path= '/content/saved_samples'\n",
        "        )\n",
        "\n",
        "  # print evaluation\n",
        "  total_instances = []\n",
        "  precisions = []\n",
        "  for label, (average_precision, num_annotations) in average_precisions.items():\n",
        "    print('{:.0f} instances of class'.format(num_annotations),\n",
        "    generator.label_to_name(label), 'with average precision: {:.4f}'.format(average_precision))\n",
        "    total_instances.append(num_annotations)\n",
        "    precisions.append(average_precision)\n",
        "\n",
        "  if sum(total_instances) == 0:\n",
        "    print('No test instances found.')\n",
        "    return\n",
        "\n",
        "  print('Inference time for {:.0f} images: {:.4f}'.format(generator.size(), inference_time))\n",
        "\n",
        "  print('mAP using the weighted average of precisions among classes: {:.4f}'.format(sum([a * b for a, b in zip(total_instances, precisions)]) / sum(total_instances)))\n",
        "  print('mAP: {:.4f}'.format(sum(precisions) / sum(x > 0 for x in total_instances)))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABfa4b44784Q"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFFPSLzyaAtb"
      },
      "source": [
        "def predict(image):\n",
        "  image = preprocess_image(image.copy())\n",
        "  image, scale = resize_image(image)\n",
        "\n",
        "  boxes, scores, labels = model.predict_on_batch(\n",
        "    np.expand_dims(image, axis=0)\n",
        "  )\n",
        "\n",
        "  boxes /= scale\n",
        "\n",
        "  return boxes, scores, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOQDuwCepwfg"
      },
      "source": [
        "THRES_SCORE = 0.5\n",
        "\n",
        "def draw__detections(image, boxes, scores, labels, generator):\n",
        "\n",
        "  #print(boxes[0], scores[0], labels[0])\n",
        "  for box, score, label in zip(boxes[0], scores, labels[0]):\n",
        "    \n",
        "    if score < THRES_SCORE:\n",
        "        break\n",
        "\n",
        "    color = label_color(label)\n",
        "    #print(box)\n",
        "    #print(score)\n",
        "    #print(label,\" \", labels_to_names[label-1])\n",
        "\n",
        "    b = box.astype(int)\n",
        "    draw_box(image, b, color=color)\n",
        "\n",
        "    caption = \"{} {:.3f}\".format(generator.label_to_name(label), score)\n",
        "    draw_caption(image, b, caption)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQrkZ06hRXuM"
      },
      "source": [
        "def show_detected_objects(image_row):\n",
        "  img_path = image_row.image_name\n",
        "  \n",
        "  image = read_image_bgr(img_path)\n",
        "\n",
        "  boxes, scores, labels = predict(image)\n",
        "\n",
        "  draw = image.copy()\n",
        "  draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  #loop\n",
        "  true_box = [\n",
        "    image_row.x_min, image_row.y_min, image_row.x_max, image_row.y_max\n",
        "  ]\n",
        "  #print(true_box)\n",
        "  draw_box(draw, true_box, color=(255, 255, 0))\n",
        "  draw_detections(draw, boxes, scores, labels)\n",
        "\n",
        "  plt.axis('off')\n",
        "  plt.imshow(draw)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJN_m6WemzJ5"
      },
      "source": [
        "#Unzip blender dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU6guAWUm6xt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05454b79-83ae-4dda-b0c2-4af035bdadda"
      },
      "source": [
        "!unzip /content/sun90-20210510T124021Z-001.zip -d /content/sun90"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/sun90-20210510T124021Z-001.zip, /content/sun90-20210510T124021Z-001.zip.zip or /content/sun90-20210510T124021Z-001.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKxmjECOn7S1"
      },
      "source": [
        "ANNOTATIONS_FILE = '/content/sun90/sun90/sun90.csv'\n",
        "CLASSES_FILE = '/content/sun90/sun90/labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN5jYnxPoQLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b100ba9-8a3b-408c-ce91-5505865ca46c"
      },
      "source": [
        "%cd - \n",
        "%cd sun90/\n",
        "%cd sun90/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "[Errno 2] No such file or directory: 'sun90/'\n",
            "/content\n",
            "[Errno 2] No such file or directory: 'sun90/'\n",
            "/content\n",
            "keras-retinanet  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuO9o8gFYh1o"
      },
      "source": [
        "#Download the EMT dataset from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFCAVNSbaogH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/MyDrive/EMTD.zip -d /content/EMTD_ALL\n",
        "\n",
        "\n",
        "#ANNOTATIONS_FILE = '/content/EMTD_ALL/EMTD/Detection/GT39_val_2.csv'\n",
        "#CLASSES_FILE = '/content/EMTD_ALL/EMTD/Detection/labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHMFC096_grA"
      },
      "source": [
        "!unzip /content/day-20210607T222247Z-001.zip -d /content/Day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RT5PCMUl6wg"
      },
      "source": [
        "ANNOTATIONS_FILE = '/content/EMTD_ALL/EMTD/Detection/GT39_test420.csv'\n",
        "CLASSES_FILE = '/content/EMTD_ALL/EMTD/Detection/labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjNzyV7h0_Wn"
      },
      "source": [
        "ANNOTATIONS_FILE ='/content/Day/day/Daynoblur.csv'\n",
        "CLASSES_FILE = '/content/Day/day/labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtQLPpGSbxPA"
      },
      "source": [
        "%cd -\n",
        "%cd EMTD_ALL/\n",
        "%cd EMTD/\n",
        "%cd Detection/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3yL51zmIJyT"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymA46U5WWMuP"
      },
      "source": [
        "main('csv',  ANNOTATIONS_FILE, CLASSES_FILE, '/content/drive/MyDrive/Resnet_Models/resnet50_csv_75.h5', '/content/saved_samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHY63XPZp_iH"
      },
      "source": [
        "!zip -r /content/EMTVD_samples.zip /content/saved_samples"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}