{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Debug.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFHRR7GLylSc"
      },
      "source": [
        "#Altered Notebook implementation of Debug.py from keras-retinanet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQYfdcUKXx1C"
      },
      "source": [
        "#Download keras-retinannet and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4dOT5rAuJnu"
      },
      "source": [
        "!git clone https://github.com/fizyr/keras-retinanet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY8Deq3muMLt"
      },
      "source": [
        "%cd keras-retinanet/\n",
        "\n",
        "!pip install keras_retinanet\n",
        "!pip install keras-resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxcI_VbKuPnI"
      },
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvHaP4TIYItx"
      },
      "source": [
        "#Visulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ZXST7AiKJz"
      },
      "source": [
        "from keras_retinanet.utils.anchors import compute_overlap\n",
        "#from keras_retinanet.utils.visualization import draw_detections, draw_annotations\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import progressbar\n",
        "assert(callable(progressbar.progressbar)), \"Using wrong progressbar module, install 'progressbar2' instead.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfnQlwnZ1ZYL"
      },
      "source": [
        "\n",
        "from  keras_retinanet.utils.colors import label_color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omEU3x37y11x"
      },
      "source": [
        "def draw_caption(image, box, caption):\n",
        "    \"\"\" Draws a caption above the box in an image.\n",
        "\n",
        "    # Arguments\n",
        "        image   : The image to draw on.\n",
        "        box     : A list of 4 elements (x1, y1, x2, y2).\n",
        "        caption : String containing the text to draw.\n",
        "    \"\"\"\n",
        "    b = np.array(box).astype(int)\n",
        "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)\n",
        "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2qAtxdPyxSG"
      },
      "source": [
        "def draw_box(image, box, color, thickness=2):\n",
        "    \"\"\" Draws a box on an image with a given color.\n",
        "\n",
        "    # Arguments\n",
        "        image     : The image to draw on.\n",
        "        box       : A list of 4 elements (x1, y1, x2, y2).\n",
        "        color     : The color of the box.\n",
        "        thickness : The thickness of the lines to draw a box with.\n",
        "    \"\"\"\n",
        "    b = np.array(box).astype(int)\n",
        "    cv2.rectangle(image, (b[0], b[1]), (b[2], b[3]), color, thickness, cv2.LINE_AA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzbuPFVAyrI9"
      },
      "source": [
        "def draw_detections(image, boxes, scores, labels, color=None, label_to_name=None, score_threshold=0.5):\n",
        "    \"\"\" Draws detections in an image.\n",
        "\n",
        "    # Arguments\n",
        "        image           : The image to draw on.\n",
        "        boxes           : A [N, 4] matrix (x1, y1, x2, y2).\n",
        "        scores          : A list of N classification scores.\n",
        "        labels          : A list of N labels.\n",
        "        color           : The color of the boxes. By default the color from keras_retinanet.utils.colors.label_color will be used.\n",
        "        label_to_name   : (optional) Functor for mapping a label to a name.\n",
        "        score_threshold : Threshold used for determining what detections to draw.\n",
        "    \"\"\"\n",
        "    selection = np.where(scores > score_threshold)[0]\n",
        "\n",
        "    for i in selection:\n",
        "        c = color if color is not None else label_color(labels[i])\n",
        "        draw_box(image, boxes[i, :], color=c)\n",
        "\n",
        "        # draw labels\n",
        "        caption = (label_to_name(labels[i]) if label_to_name else labels[i]) + ': {0:.2f}'.format(scores[i])\n",
        "        draw_caption(image, boxes[i, :], caption)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZedggRwZynUl"
      },
      "source": [
        "def draw_annotations(image, annotations, color=(0, 255, 0), label_to_name=None):\n",
        "    \"\"\" Draws annotations in an image.\n",
        "\n",
        "    # Arguments\n",
        "        image         : The image to draw on.\n",
        "        annotations   : A [N, 5] matrix (x1, y1, x2, y2, label) or dictionary containing bboxes (shaped [N, 4]) and labels (shaped [N]).\n",
        "        color         : The color of the boxes. By default the color from keras_retinanet.utils.colors.label_color will be used.\n",
        "        label_to_name : (optional) Functor for mapping a label to a name.\n",
        "    \"\"\"\n",
        "    if isinstance(annotations, np.ndarray):\n",
        "        annotations = {'bboxes': annotations[:, :4], 'labels': annotations[:, 4]}\n",
        "\n",
        "    assert('bboxes' in annotations)\n",
        "    assert('labels' in annotations)\n",
        "    assert(annotations['bboxes'].shape[0] == annotations['labels'].shape[0])\n",
        "\n",
        "    for i in range(annotations['bboxes'].shape[0]):\n",
        "        label   = annotations['labels'][i]\n",
        "        c       = color if color is not None else label_color(label)\n",
        "        caption = '{}'.format(label_to_name(label) if label_to_name else label)\n",
        "        draw_caption(image, annotations['bboxes'][i], caption)\n",
        "        draw_box(image, annotations['bboxes'][i], color=c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4mFr2FVClyt"
      },
      "source": [
        "def _get_annotations(generator):\n",
        "    \"\"\" Get the ground truth annotations from the generator.\n",
        "\n",
        "    The result is a list of lists such that the size is:\n",
        "        all_detections[num_images][num_classes] = annotations[num_detections, 5]\n",
        "\n",
        "    # Arguments\n",
        "        generator : The generator used to retrieve ground truth annotations.\n",
        "    # Returns\n",
        "        A list of lists containing the annotations for each image in the generator.\n",
        "    \"\"\"\n",
        "    all_annotations = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
        "\n",
        "    for i in progressbar.progressbar(range(generator.size()), prefix='Parsing annotations: '):\n",
        "        # load the annotations\n",
        "        annotations = generator.load_annotations(i)\n",
        "\n",
        "        # copy detections to all_annotations\n",
        "        for label in range(generator.num_classes()):\n",
        "            if not generator.has_label(label):\n",
        "                continue\n",
        "\n",
        "            all_annotations[i][label] = annotations['bboxes'][annotations['labels'] == label, :].copy()\n",
        "\n",
        "    return all_annotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw9FN9cR473U"
      },
      "source": [
        "def draw_boxes(image, boxes, color, thickness=2):\n",
        "    \"\"\" Draws boxes on an image with a given color.\n",
        "\n",
        "    # Arguments\n",
        "        image     : The image to draw on.\n",
        "        boxes     : A [N, 4] matrix (x1, y1, x2, y2).\n",
        "        color     : The color of the boxes.\n",
        "        thickness : The thickness of the lines to draw boxes with.\n",
        "    \"\"\"\n",
        "    for b in boxes:\n",
        "        draw_box(image, b, color, thickness=thickness)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-_ZCrjD25gl"
      },
      "source": [
        "#Debug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGwuTMprXVmW"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "\n",
        "# Set keycodes for changing images\n",
        "# 81, 83 are left and right arrows on linux in Ascii code (probably not needed)\n",
        "# 65361, 65363 are left and right arrows in linux\n",
        "# 2424832, 2555904 are left and right arrows on Windows\n",
        "# 110, 109 are 'n' and 'm' on mac, windows, linux\n",
        "# (unfortunately arrow keys not picked up on mac)\n",
        "leftkeys = (81, 110, 65361, 2424832)\n",
        "rightkeys = (83, 109, 65363, 2555904)\n",
        "\n",
        "# Allow relative imports when being executed as script.\n",
        "'''\n",
        "if __name__ == \"__main__\" and __package__ is None:\n",
        "    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))\n",
        "    import keras_retinanet.bin  # noqa: F401\n",
        "    __package__ = \"keras_retinanet.bin\"\n",
        "'''\n",
        "# Change these to absolute imports if you copy this script outside the keras_retinanet package.\n",
        "from keras_retinanet.preprocessing.pascal_voc import PascalVocGenerator\n",
        "from keras_retinanet.preprocessing.csv_generator import CSVGenerator\n",
        "from keras_retinanet.preprocessing.kitti import KittiGenerator\n",
        "from keras_retinanet.preprocessing.open_images import OpenImagesGenerator\n",
        "from keras_retinanet.utils.anchors import anchors_for_shape, compute_gt_annotations\n",
        "from keras_retinanet.utils.config import read_config_file, parse_anchor_parameters, parse_pyramid_levels\n",
        "from keras_retinanet.utils.image import random_visual_effect_generator\n",
        "from keras_retinanet.utils.tf_version import check_tf_version\n",
        "from keras_retinanet.utils.transform import random_transform_generator\n",
        "from keras_retinanet.utils.visualization import draw_annotations, draw_boxes, draw_caption\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR5cLytJXn8x"
      },
      "source": [
        "def create_generator(dataset_type, annotations, classes):\n",
        "    \"\"\" Create the data generators.\n",
        "    Args:\n",
        "        args: parseargs arguments object.\n",
        "    \"\"\"\n",
        "    '''\n",
        "    common_args = {\n",
        "        'config'           : args.config,\n",
        "        'image_min_side'   : args.image_min_side,\n",
        "        'image_max_side'   : args.image_max_side,\n",
        "        'group_method'     : args.group_method\n",
        "    }\n",
        "    '''\n",
        "\n",
        "    # create random transform generator for augmenting training data\n",
        "    transform_generator = random_transform_generator(\n",
        "        min_rotation=-0.1,\n",
        "        max_rotation=0.1,\n",
        "        min_translation=(-0.1, -0.1),\n",
        "        max_translation=(0.1, 0.1),\n",
        "        min_shear=-0.1,\n",
        "        max_shear=0.1,\n",
        "        min_scaling=(0.9, 0.9),\n",
        "        max_scaling=(1.1, 1.1),\n",
        "        flip_x_chance=0.5,\n",
        "        flip_y_chance=0.5,\n",
        "    )\n",
        "\n",
        "    visual_effect_generator = random_visual_effect_generator(\n",
        "        contrast_range=(0.9, 1.1),\n",
        "        brightness_range=(-.1, .1),\n",
        "        hue_range=(-0.05, 0.05),\n",
        "        saturation_range=(0.95, 1.05)\n",
        "    )\n",
        "\n",
        "    if dataset_type == 'coco':\n",
        "        # import here to prevent unnecessary dependency on cocoapi\n",
        "        from ..preprocessing.coco import CocoGenerator\n",
        "\n",
        "        generator = CocoGenerator(\n",
        "            args.coco_path,\n",
        "            args.coco_set,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "    elif dataset_type == 'pascal':\n",
        "        generator = PascalVocGenerator(\n",
        "            args.pascal_path,\n",
        "            args.pascal_set,\n",
        "            image_extension=args.image_extension,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "    elif dataset_type == 'csv':\n",
        "        generator = CSVGenerator(\n",
        "            annotations,\n",
        "            classes,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            #**common_args\n",
        "        )\n",
        "    elif dataset_type == 'oid':\n",
        "        generator = OpenImagesGenerator(\n",
        "            args.main_dir,\n",
        "            subset=args.subset,\n",
        "            version=args.version,\n",
        "            labels_filter=args.labels_filter,\n",
        "            parent_label=args.parent_label,\n",
        "            annotation_cache_dir=args.annotation_cache_dir,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "    elif dataset_type == 'kitti':\n",
        "        generator = KittiGenerator(\n",
        "            args.kitti_path,\n",
        "            subset=args.subset,\n",
        "            transform_generator=transform_generator,\n",
        "            visual_effect_generator=visual_effect_generator,\n",
        "            **common_args\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))\n",
        "\n",
        "    return generator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MieSBofoYMrl"
      },
      "source": [
        "def parse_args(args):\n",
        "    \"\"\" Parse the arguments.\n",
        "    \"\"\"\n",
        "    parser     = argparse.ArgumentParser(description='Debug script for a RetinaNet network.')\n",
        "    subparsers = parser.add_subparsers(help='Arguments for specific dataset types.', dest='dataset_type')\n",
        "    subparsers.required = True\n",
        "\n",
        "    coco_parser = subparsers.add_parser('coco')\n",
        "    coco_parser.add_argument('coco_path',  help='Path to dataset directory (ie. /tmp/COCO).')\n",
        "    coco_parser.add_argument('--coco-set', help='Name of the set to show (defaults to val2017).', default='val2017')\n",
        "\n",
        "    pascal_parser = subparsers.add_parser('pascal')\n",
        "    pascal_parser.add_argument('pascal_path', help='Path to dataset directory (ie. /tmp/VOCdevkit).')\n",
        "    pascal_parser.add_argument('--pascal-set',  help='Name of the set to show (defaults to test).', default='test')\n",
        "    pascal_parser.add_argument('--image-extension',   help='Declares the dataset images\\' extension.', default='.jpg')\n",
        "\n",
        "    kitti_parser = subparsers.add_parser('kitti')\n",
        "    kitti_parser.add_argument('kitti_path', help='Path to dataset directory (ie. /tmp/kitti).')\n",
        "    kitti_parser.add_argument('subset', help='Argument for loading a subset from train/val.')\n",
        "\n",
        "    def csv_list(string):\n",
        "        return string.split(',')\n",
        "\n",
        "    oid_parser = subparsers.add_parser('oid')\n",
        "    oid_parser.add_argument('main_dir', help='Path to dataset directory.')\n",
        "    oid_parser.add_argument('subset', help='Argument for loading a subset from train/validation/test.')\n",
        "    oid_parser.add_argument('--version',  help='The current dataset version is v4.', default='v4')\n",
        "    oid_parser.add_argument('--labels-filter',  help='A list of labels to filter.', type=csv_list, default=None)\n",
        "    oid_parser.add_argument('--annotation-cache-dir', help='Path to store annotation cache.', default='.')\n",
        "    oid_parser.add_argument('--parent-label', help='Use the hierarchy children of this label.', default=None)\n",
        "\n",
        "    csv_parser = subparsers.add_parser('csv')\n",
        "    csv_parser.add_argument('annotations', help='Path to CSV file containing annotations for evaluation.')\n",
        "    csv_parser.add_argument('classes',     help='Path to a CSV file containing class label mapping.')\n",
        "\n",
        "    parser.add_argument('--no-resize', help='Disable image resizing.', dest='resize', action='store_false')\n",
        "    parser.add_argument('--anchors', help='Show positive anchors on the image.', action='store_true')\n",
        "    parser.add_argument('--display-name', help='Display image name on the bottom left corner.', action='store_true')\n",
        "    parser.add_argument('--show-annotations', help='Show annotations on the image. Green annotations have anchors, red annotations don\\'t and therefore don\\'t contribute to training.', action='store_true')\n",
        "    parser.add_argument('--random-transform', help='Randomly transform image and annotations.', action='store_true')\n",
        "    parser.add_argument('--image-min-side', help='Rescale the image so the smallest side is min_side.', type=int, default=800)\n",
        "    parser.add_argument('--image-max-side', help='Rescale the image if the largest side is larger than max_side.', type=int, default=1333)\n",
        "    parser.add_argument('--config', help='Path to a configuration parameters .ini file.')\n",
        "    parser.add_argument('--no-gui', help='Do not open a GUI window. Save images to an output directory instead.', action='store_true')\n",
        "    parser.add_argument('--output-dir', help='The output directory to save images to if --no-gui is specified.', default='.')\n",
        "    parser.add_argument('--flatten-output', help='Flatten the folder structure of saved output images into a single folder.', action='store_true')\n",
        "    parser.add_argument('--group-method', help='Determines how images are grouped together', type=str, default='ratio', choices=['none', 'random', 'ratio'])\n",
        "\n",
        "    return parser.parse_args(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCJlQLaBYUgf"
      },
      "source": [
        "def run(generator, anchor_params, pyramid_levels):\n",
        "    \"\"\" Main loop.\n",
        "    Args\n",
        "        generator: The generator to debug.\n",
        "        args: parseargs args object.\n",
        "    \"\"\"\n",
        "    # display images, one at a time\n",
        "    i = 0\n",
        "    resize = False\n",
        "    anchors = True\n",
        "    show_annotations = True\n",
        "    display_name = True\n",
        "    no_gui = True\n",
        "    random_transform = False\n",
        "    output_dir = '/content/saved_samples5'\n",
        "    while True:\n",
        "        # load the data\n",
        "        image       = generator.load_image(i)\n",
        "        annotations = generator.load_annotations(i)\n",
        "        print(generator.image_path(i))\n",
        "        if len(annotations['labels']) > 0 :\n",
        "            # apply random transformations\n",
        "            if random_transform == False:\n",
        "                image, annotations = generator.random_transform_group_entry(image, annotations)\n",
        "                image, annotations = generator.random_visual_effect_group_entry(image, annotations)\n",
        "\n",
        "            # resize the image and annotations\n",
        "            if resize:\n",
        "                image, image_scale = generator.resize_image(image)\n",
        "                annotations['bboxes'] *= image_scale\n",
        "\n",
        "            anchors = anchors_for_shape(image.shape, anchor_params=anchor_params, pyramid_levels=pyramid_levels)\n",
        "            positive_indices, _, max_indices = compute_gt_annotations(anchors, annotations['bboxes'])\n",
        "            #print(\"positive & max indices\",positive_indices, max_indices)\n",
        "\n",
        "            #print(annotations['bboxes'][max_indices[positive_indices], :])\n",
        "            bboxes = annotations['bboxes'][max_indices[positive_indices], :]\n",
        "            '''\n",
        "            if bboxes.any():\n",
        "              pass\n",
        "            else:\n",
        "              print(generator.image_path(i))\n",
        "\n",
        "            i += 1\n",
        "            if i == generator.size():  # have written all images\n",
        "                break\n",
        "            else:\n",
        "                continue\n",
        "            '''\n",
        "            # draw anchors on the image\n",
        "            if anchors.any():\n",
        "              draw_boxes(image, anchors[positive_indices], (255, 255, 0), thickness=1)\n",
        "\n",
        "            # draw annotations on the image\n",
        "            if show_annotations:\n",
        "              print(\"draw annotations\")\n",
        "              # draw annotations in red\n",
        "              draw_annotations(image, annotations, color=(0, 0, 255), label_to_name=generator.label_to_name)\n",
        "              # draw regressed anchors in green to override most red annotations\n",
        "              # result is that annotations without anchors are red, with anchors are green\n",
        "              draw_boxes(image, annotations['bboxes'][max_indices[positive_indices], :], (0, 255, 0))\n",
        "\n",
        "            # display name on the image\n",
        "            if display_name:\n",
        "              draw_caption(image, [0, image.shape[0]], os.path.basename(generator.image_path(i)))\n",
        "          \n",
        "        # write to file and advance if no-gui selected\n",
        "        if no_gui:\n",
        "          output_path = make_output_path(output_dir, generator.image_path(i), flatten=True)\n",
        "          os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "          cv2.imwrite(output_path, image)\n",
        "          i += 1\n",
        "          if i == generator.size():  # have written all images\n",
        "            break\n",
        "          else:\n",
        "            continue\n",
        "      \n",
        "        # if we are using the GUI, then show an image\n",
        "        cv2.imshow('Image', image)\n",
        "        key = cv2.waitKeyEx()\n",
        "\n",
        "        # press right for next image and left for previous (linux or windows, doesn't work for macOS)\n",
        "        # if you run macOS, press \"n\" or \"m\" (will also work on linux and windows)\n",
        "\n",
        "        if key in rightkeys:\n",
        "            i = (i + 1) % generator.size()\n",
        "        if key in leftkeys:\n",
        "            i -= 1\n",
        "            if i < 0:\n",
        "                i = generator.size() - 1\n",
        "\n",
        "        # press q or Esc to quit\n",
        "        if (key == ord('q')) or (key == 27):\n",
        "            return False\n",
        "        \n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2KsNfbTYVhC"
      },
      "source": [
        "def make_output_path(output_dir, image_path, flatten = False):\n",
        "    \"\"\" Compute the output path for a debug image. \"\"\"\n",
        "\n",
        "    # If the output hierarchy is flattened to a single folder, throw away all leading folders.\n",
        "    if flatten:\n",
        "        path = os.path.basename(image_path)\n",
        "\n",
        "    # Otherwise, make sure absolute paths are taken relative to the filesystem root.\n",
        "    else:\n",
        "        # Make sure to drop drive letters on Windows, otherwise relpath wil fail.\n",
        "        _, path = os.path.splitdrive(image_path)\n",
        "        if os.path.isabs(path):\n",
        "            path = os.path.relpath(path, '/')\n",
        "\n",
        "    # In all cases, append \"_debug\" to the filename, before the extension.\n",
        "    base, extension = os.path.splitext(path)\n",
        "    path = base + \"_debug\" + extension\n",
        "\n",
        "    # Finally, join the whole thing to the output directory.\n",
        "    return os.path.join(output_dir, path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn7Ckm0DYcFt"
      },
      "source": [
        "def main(dataset_type, annotations, classes, no_gui):\n",
        "    # parse arguments\n",
        "    '''\n",
        "    if args is None:\n",
        "        args = sys.argv[1:]\n",
        "    args = parse_args(args)\n",
        "    '''\n",
        "    # make sure tensorflow is the minimum required version\n",
        "    check_tf_version()\n",
        "\n",
        "    # create the generator\n",
        "    generator = create_generator(dataset_type, annotations, classes)\n",
        "\n",
        "    # optionally load config parameters\n",
        "    config = read_config_file('/content/Config/config.ini')\n",
        "    '''\n",
        "    if args.config:\n",
        "        args.config = read_config_file(args.config)\n",
        "    '''\n",
        "\n",
        "    # optionally load anchor parameters\n",
        "    anchor_params = None\n",
        "    \n",
        "    if config and 'anchor_parameters' in config:\n",
        "        anchor_params = parse_anchor_parameters(config)\n",
        "    \n",
        "    pyramid_levels = None\n",
        "    '''\n",
        "    if args.config and 'pyramid_levels' in args.config:\n",
        "        pyramid_levels = parse_pyramid_levels(args.config)\n",
        "    '''\n",
        "    # create the display window if necessary\n",
        "    if no_gui == 'false':\n",
        "        cv2.namedWindow('Image', cv2.WINDOW_NORMAL)\n",
        "\n",
        "    run(generator, anchor_params=anchor_params, pyramid_levels=pyramid_levels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "209-dL7cg7Ne"
      },
      "source": [
        "#Unzip virtual dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJgSoxM-g_VD"
      },
      "source": [
        "!unzip /content/day-20210429T211636Z-001.zip -d /content/EMVTD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykk3JUEwh7j_"
      },
      "source": [
        "%cd - \n",
        "%cd EMVTD/\n",
        "%cd day/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmCMrU7ih-fE"
      },
      "source": [
        "ANNOTATIONS_FILE = '/content/EMVTD/day/GTV39_test.csv' \n",
        "CLASSES_FILE = '/content/EMVTD/day/labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuO9o8gFYh1o"
      },
      "source": [
        "#Download the EMT dataset from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFCAVNSbaogH"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#!unzip /content/drive/MyDrive/EMTD.zip -d /content/EMTD_ALL\n",
        "\n",
        "\n",
        "#ANNOTATIONS_FILE = '/content/EMTD_ALL/EMTD/Detection/GTV39_test.csv' \n",
        "#CLASSES_FILE = '/content/EMTD_ALL/EMTD/Detection/labels.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s7epxXghd0d"
      },
      "source": [
        "%cd - \n",
        "%cd EMVTD/\n",
        "%cd day/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtQLPpGSbxPA"
      },
      "source": [
        "%cd -\n",
        "%cd EMTD_ALL/\n",
        "%cd EMTD/\n",
        "%cd Detection/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN5d3pHihZCg"
      },
      "source": [
        "main('csv',  ANNOTATIONS_FILE, CLASSES_FILE, 'true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqihf3d5jKPC"
      },
      "source": [
        "!zip -r /content/debugged_EMTD5.zip /content/saved_samples5"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}